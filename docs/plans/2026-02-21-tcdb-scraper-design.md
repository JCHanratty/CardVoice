# TCDB-to-CardVoice Scraper Design

**Date:** 2026-02-21
**Status:** Approved

## Goal

Build a Python scraper that extracts baseball card set checklists (base cards, inserts, parallels, thumbnails) from tcdb.com and outputs a CardVoice-compatible catalog database. A separate migrator script logs into a TCDB account and maps owned cards to CardVoice quantities.

## Deliverables

1. **scraper.py** — Builds `tcdb-catalog.db` + downloads thumbnail images
2. **migrator.py** — Logs into TCDB, extracts owned card data for qty import
3. **CardVoice changes** — Schema addition (`image_path`), catalog merge update, qty import endpoint

## Project Structure

```
tcdb-scraper/
├── .env                    # TCDB_USER, TCDB_PASS
├── requirements.txt        # requests, beautifulsoup4, python-dotenv
├── scraper.py              # Main scraper — builds tcdb-catalog.db
├── migrator.py             # Collection migrator — sets qty from TCDB account
├── checkpoint.json         # Resume state (auto-generated)
├── my_sets.json            # Owned set IDs (generated by migrator --discover)
├── output/
│   ├── tcdb-catalog.db     # SQLite output matching CardVoice schema
│   └── images/             # Downloaded card thumbnails
│       └── {set_id}/       # Organized by set
│           └── {card_id}.jpg
```

## Scraper Flow (scraper.py)

### Phase 1: Discover Sets

- Iterate `GET /ViewAll.cfm/sp/Baseball/year/{year}` from 2026 downward
- Extract set name, set ID (from URL), year, card count
- Save full set list to checkpoint.json

### Phase 2: Scrape Each Set (priority order)

For each set:
1. `GET /ViewSet.cfm/sid/{set_id}`
2. Parse base cards: card number, player, team, RC/SP flags
3. Parse insert sections: name, card list, odds
4. Parse parallel info: name, print run, exclusivity
5. Handle pagination if set spans multiple pages
6. Download card thumbnail images to `output/images/{set_id}/`
7. Write data to `tcdb-catalog.db`
8. Update checkpoint.json (mark set complete)
9. Random delay 3-8 seconds between every HTTP request

### Rate Limiting & Human-Like Behavior

- Random 3-8 second delay between all requests
- Realistic User-Agent header
- `requests.Session()` for cookie persistence
- Runs continuously until complete

## Priority Scheduling

```
Priority 1: Sets the user owns cards in, 2026 → oldest
Priority 2: All remaining sets, 2026 → oldest
```

The migrator's `--discover` mode generates `my_sets.json` which the scraper reads for priority ordering.

## Data Mapping: TCDB → CardVoice Schema

| TCDB Field | CardVoice Table | CardVoice Column |
|-----------|----------------|-----------------|
| Set name | `card_sets` | `name` |
| Year | `card_sets` | `year` |
| Brand (from name) | `card_sets` | `brand` |
| "Baseball" | `card_sets` | `sport` |
| Card count | `card_sets` | `total_cards` |
| Card number | `cards` | `card_number` |
| Player name | `cards` | `player` |
| Team | `cards` | `team` |
| RC/SP flags | `cards` | `rc_sp` |
| Insert/subset name | `cards` | `insert_type` + `set_insert_types` |
| Parallel name | `cards` | `parallel` + `set_parallels` |
| Qty | `cards` | `qty` = 0 (always, catalog only) |
| Thumbnail path | `cards` | `image_path` (new column) |
| Insert section name | `set_insert_types` | `name` |
| Insert card count | `set_insert_types` | `card_count` |
| Insert odds | `set_insert_types` | `odds` |
| Parallel name | `set_parallels` | `name` |
| Print run | `set_parallels` | `print_run`, `serial_max` |
| Exclusivity | `set_parallels` | `exclusive` |

Brand extraction parses from set name: "2025 Topps Chrome" → brand = "Topps".

## Migrator Flow (migrator.py)

### Mode 1: Discovery Only (`--discover`)

1. Login to TCDB with credentials from `.env`
2. Navigate to user's collection pages
3. Extract list of all sets with cards + card counts
4. Save to `my_sets.json`

### Mode 2: Full Migration (`--migrate`)

1. Login to TCDB
2. For each set in collection:
   - Scrape user's specific card list (owned cards, quantities)
   - Match against cards in `tcdb-catalog.db`
   - Output `qty_updates.json` mapping card IDs → quantities
3. Import into CardVoice via `POST /api/import-qty`

## Typical Workflow

1. `python migrator.py --discover` → generates `my_sets.json`
2. `python scraper.py` → builds catalog DB, prioritizes owned sets
3. Import `tcdb-catalog.db` into CardVoice (catalog merge)
4. `python migrator.py --migrate` → generates qty updates
5. Apply qty updates to CardVoice

## Error Handling

| Scenario | Behavior |
|----------|----------|
| HTTP 403/429 | Wait 60s, retry up to 3x, then skip set and log warning |
| HTTP 5xx | Wait 30s, retry up to 3x |
| Connection timeout | 30s timeout per request, retry once |
| Parse failure | Log URL + HTML snippet to errors.log, skip card, continue |
| Image download fails | Log it, set image_path = '', continue |
| Script interrupted | Checkpoint saved after each set — resume picks up cleanly |
| Login fails | Clear error message, exit immediately |
| Duplicate on resume | Checkpoint tracks completed set IDs, skips them |

Logging: Python `logging` module — INFO to console, DEBUG to `scraper.log`, WARNING/ERROR for issues.

## CardVoice Changes Required

### 1. Schema addition (server/db.js)

Add `image_path TEXT DEFAULT ''` column to `cards` table.

### 2. Catalog merge update (server/catalog-merge.js)

- Handle new `image_path` column during merge
- Copy image files from staging folder to `%APPDATA%/CardVoice/images/`

### 3. Qty import endpoint (server/routes.js)

New `POST /api/import-qty` endpoint — accepts `qty_updates.json` and applies quantities to matching cards.

### 4. Frontend display (optional, later)

- Show thumbnail in SetDetail.jsx card rows
- Lazy-load images for performance

## Dependencies

```
requests>=2.31.0
beautifulsoup4>=4.12.0
python-dotenv>=1.0.0
```

All other dependencies (sqlite3, logging, json, time, random, os) are Python stdlib.
